{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02cc6ac5-209d-460b-8a73-ef063a24aa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw features data:    age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   63    1   1       145   233    1        2      150      0      2.3      3   \n",
      "1   67    1   4       160   286    0        2      108      1      1.5      2   \n",
      "2   67    1   4       120   229    0        2      129      1      2.6      2   \n",
      "3   37    1   3       130   250    0        0      187      0      3.5      3   \n",
      "4   41    0   2       130   204    0        2      172      0      1.4      1   \n",
      "\n",
      "    ca  thal  \n",
      "0  0.0   6.0  \n",
      "1  3.0   3.0  \n",
      "2  2.0   7.0  \n",
      "3  0.0   3.0  \n",
      "4  0.0   3.0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "# Fetch dataset\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "print(\"Raw features data:\", heart_disease.data.features[:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "244ecda9-f8a3-4459-9ca1-1f482c78128d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   63    1   0       145    64    1        2       49      0       22      2   \n",
      "1   67    1   3       160   111    0        2       10      1       15      1   \n",
      "2   67    1   3       120    60    0        2       29      1       25      1   \n",
      "3   37    1   2       130    80    0        0       84      0       32      2   \n",
      "4   41    0   1       130    35    0        2       71      0       14      0   \n",
      "\n",
      "   ca  thal  \n",
      "0   0     1  \n",
      "1   3     0  \n",
      "2   2     2  \n",
      "3   0     0  \n",
      "4   0     0  \n"
     ]
    }
   ],
   "source": [
    "#2(5 pts) Clean your dataset to turn categorical values into numerical ones. One-hot encoding is likely the answer, but it depends on the dataset. Your data may have ordinal columns, for example where one-hot encoding is not as appropriate.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "X = pd.DataFrame(heart_disease.data.features, columns=heart_disease.data.feature_names)\n",
    "y = pd.Series(heart_disease.data.targets.squeeze(), name='Outcome')\n",
    "class_labels = LabelEncoder()\n",
    "X['sex'] = class_labels.fit_transform(X['sex'].values)\n",
    "X['cp'] = class_labels.fit_transform(X['cp'].values)\n",
    "X['chol'] = class_labels.fit_transform(X['chol'].values)\n",
    "X['fbs'] = class_labels.fit_transform(X['fbs'].values)\n",
    "X['restecg'] = class_labels.fit_transform(X['restecg'].values)\n",
    "X['thalach'] = class_labels.fit_transform(X['thalach'].values)\n",
    "X['exang'] = class_labels.fit_transform(X['exang'].values)\n",
    "X['oldpeak'] = class_labels.fit_transform(X['oldpeak'].values)\n",
    "X['slope'] = class_labels.fit_transform(X['slope'].values)\n",
    "X['ca'] = class_labels.fit_transform(X['ca'].values)\n",
    "X['thal'] = class_labels.fit_transform(X['thal'].values)\n",
    "\n",
    "print(X.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e4d4a01-fa03-4c95-9782-f220c2cb69bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.4469980118591779\n"
     ]
    }
   ],
   "source": [
    "#3(3 pts) Perform univariate linear regression on the dataset. Select your variable to predict. How well did this model perform? Is this a good approach for this dataset? Why or why not? \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "X = pd.DataFrame(heart_disease.data.features, columns=heart_disease.data.feature_names)\n",
    "y = pd.Series(heart_disease.data.targets.to_numpy().flatten(), name='Outcome')  \n",
    "valid_indices = ~y.isna()  \n",
    "x_valid = X.loc[valid_indices, 'age'].astype(float).values.ravel()  # 1D array for features\n",
    "y_valid = y[valid_indices].values.ravel()  # 1D array for target\n",
    "scaler = StandardScaler()\n",
    "x_valid = scaler.fit_transform(x_valid.reshape(-1, 1)).ravel()  # Reshape for scaling, then flatten\n",
    "\n",
    "class LinearRegressor:\n",
    "    def __init__(self, x, y, alpha=0.001, b0=0, b1=0):  # Reduce alpha\n",
    "        self.i = 0\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.alpha = alpha\n",
    "        self.b0 = b0\n",
    "        self.b1 = b1\n",
    "        if len(x) != len(y):\n",
    "            raise TypeError(\"x and y should have the same number of rows.\")\n",
    "\n",
    "    def cost_derivative(self):\n",
    "        errors = [(self.predict(xi) - yi) ** 2 for xi, yi in zip(self.x, self.y)]\n",
    "        mse = sum(errors) / len(errors)\n",
    "        return mse\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.b0 + self.b1 * x\n",
    "\n",
    "    def fit(self):\n",
    "        max_epochs = 1000\n",
    "        for _ in range(max_epochs):\n",
    "            self.update_coeff(0)\n",
    "            self.update_coeff(1)\n",
    "        return self.cost_derivative()  # Return the MSE after fitting\n",
    "\n",
    "    def update_coeff(self, i):\n",
    "        if i == 0:\n",
    "            self.b0 -= self.alpha * self.grad_fun(0)\n",
    "        elif i == 1:\n",
    "            self.b1 -= self.alpha * self.grad_fun(1)\n",
    "\n",
    "    def grad_fun(self, i):\n",
    "        return sum([\n",
    "            2 * (self.predict(xi) - yi) * (1 if i == 0 else xi)\n",
    "            for xi, yi in zip(self.x, self.y)\n",
    "        ]) / len(self.x)\n",
    "regressor = LinearRegressor(x=x_valid, y=y_valid)  \n",
    "mse = regressor.fit()  \n",
    "print(\"Mean Squared Error:\", mse)\n",
    "#all code is adapted from geeks for geeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c749426-dd0e-47a2-a22e-bf136308e381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k=1: 0.5082\n",
      "Accuracy for k=2: 0.5246\n",
      "Accuracy for k=3: 0.4918\n",
      "Accuracy for k=4: 0.5246\n",
      "Accuracy for k=5: 0.5246\n",
      "Accuracy for k=6: 0.5574\n",
      "Accuracy for k=7: 0.5410\n",
      "Accuracy for k=8: 0.5410\n",
      "Accuracy for k=9: 0.5574\n",
      "Accuracy for k=10: 0.5738\n",
      "Accuracy for k=11: 0.6066\n",
      "Accuracy for k=12: 0.5738\n",
      "Accuracy for k=13: 0.5574\n",
      "Accuracy for k=14: 0.5574\n",
      "Accuracy for k=15: 0.5574\n",
      "Accuracy for k=16: 0.5410\n",
      "Accuracy for k=17: 0.5574\n",
      "Accuracy for k=18: 0.5574\n",
      "Accuracy for k=19: 0.5410\n",
      "Accuracy for k=20: 0.5738\n",
      "Optimal k: 11 with accuracy: 0.6066\n",
      "Accuracy with KNN (optimal k): 0.6065573770491803\n"
     ]
    }
   ],
   "source": [
    "#3(8 pts) Perform KNN on this dataset. As part of this, write a function that selects the optimal value of k. How well did this model perform?\n",
    "#The model performed not too well not to bad because it was 0.60 a little better than 50 but not significant.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "X = pd.DataFrame(heart_disease.data.features, columns=heart_disease.data.feature_names)\n",
    "y = heart_disease.data.targets.squeeze()\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "def select_optimal_k(X_train, y_train, X_test, y_test, max_k):\n",
    "    accuracy_list = []\n",
    "    \n",
    "    for k in range(1, max_k + 1):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracy_list.append(accuracy)\n",
    "        print(f'Accuracy for k={k}: {accuracy:.4f}')\n",
    "\n",
    "    optimal_k = accuracy_list.index(max(accuracy_list)) + 1\n",
    "    print(f'Optimal k: {optimal_k} with accuracy: {max(accuracy_list):.4f}')\n",
    "    return optimal_k\n",
    "optimal_k = select_optimal_k(X_train_standardized, y_train, X_test_standardized, y_test, max_k=20)\n",
    "knn = KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "knn.fit(X_train_standardized, y_train)\n",
    "y_pred = knn.predict(X_test_standardized)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy with KNN (optimal k):\", accuracy)\n",
    "\n",
    "#adapted from geeks for geeks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1486d1b-f362-4390-879e-3bb0fb1448fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6065573770491803\n"
     ]
    }
   ],
   "source": [
    "#4(6 pts) Work with your dataset to perform logistic regression. How well did this perform?\n",
    "#It has a 60% value so I don't think it means very good nor very bad, so not statistically significant.\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load the UC Irvine Heart Disease dataset and preprocess it.\"\"\"\n",
    "    heart_disease = fetch_ucirepo(id=45)\n",
    "    X = pd.DataFrame(heart_disease.data.features, columns=heart_disease.data.feature_names)\n",
    "    y = heart_disease.data.targets.squeeze()  # Convert target to 1D\n",
    "    return X, y\n",
    "\n",
    "def preprocess_data(X):\n",
    "    \"\"\"Handle missing values and scale features.\"\"\"\n",
    "    imputer = SimpleImputer(strategy='mean')  \n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    sc = StandardScaler()\n",
    "    X_scaled = sc.fit_transform(X_imputed)\n",
    "    \n",
    "    return X_scaled\n",
    "\n",
    "def train_logistic_regression(X_train, y_train):\n",
    "    \"\"\"Train the Logistic Regression model.\"\"\"\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "\n",
    "def main():\n",
    "    X, y = load_and_prepare_data()\n",
    "    X_scaled = preprocess_data(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    model = train_logistic_regression(X_train, y_train)\n",
    "    evaluate_model(model, X_test, y_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b28b49ea-6979-44c9-8a60-1164cc1d1e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with normalization: 0.6065573770491803\n",
      "Confusion Matrix (normalized):\n",
      " [[31  1  1  0  0]\n",
      " [ 6  4  1  0  0]\n",
      " [ 2  1  0  4  0]\n",
      " [ 0  1  4  2  0]\n",
      " [ 0  2  0  1  0]]\n",
      "Classification Report (normalized):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86        33\n",
      "           1       0.44      0.36      0.40        11\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.29      0.29      0.29         7\n",
      "           4       1.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61        61\n",
      "   macro avg       0.51      0.32      0.31        61\n",
      "weighted avg       0.59      0.61      0.57        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5. (3 pts) Perform normalization on your dataset. Does it change the performance for 2-4? What is the best measure of performance for your dataset (accuracy or something else) and why?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "X = pd.DataFrame(heart_disease.data.features, columns=heart_disease.data.feature_names)\n",
    "y = heart_disease.data.targets.squeeze()\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42, stratify=y)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test):\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, zero_division=1)\n",
    "    print(\"Accuracy with normalization:\", accuracy)\n",
    "    print(\"Confusion Matrix (normalized):\\n\", confusion)\n",
    "    print(\"Classification Report (normalized):\\n\", report)\n",
    "train_and_evaluate_model(X_train_normalized, X_test_normalized, y_train, y_test)\n",
    "\n",
    "\n",
    "#Yes the performance does change with different classes. The f1-score changes with different classifications because the data that comes from classification can vary, \n",
    "#each class doesn't have the same data size.Also there can be false negatives's and positives. It's one of the biggest issues with classification reports,\n",
    "#but that is why we used normalization to eliminate that. The F1 score is the gold standard for performance/accuracy. All code was adapted from Joel grus network analysis and I used codes for geeks for the squeeze. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
